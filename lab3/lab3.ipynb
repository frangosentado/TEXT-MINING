{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf0bae6-3502-4a49-9d29-8307e5c69aef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Assignment 3\n",
    "\n",
    "## Question 1\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "```\n",
    " There is a correct interpretation that the sentence is positive, given by the fact that it contains the word 'love', which in the lexicon has a sentiment rating of 3.2 (close to 4), which indicates a positive sentiment.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "```\n",
    " There is a correct interpretation that the sentence is a little negative and neutral, given by the fact that it contains the word 'love', which in the lexicon has a sentiment rating of 3.2 (close to 4), but also has the negation \"don't\". This negation inverts the polarity of \"love\", that is, although \"love\" is a positive word, it is interpreted as negative one. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "```\n",
    " There is a correct interpretation that it is highly positive. It recognizes the word \"love\" as highly positive in the lexicon and also considers the presence of the emoticon \":-)\" as an intensifier, contributing to the overall positive sentiment.\n",
    "```\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "```\n",
    " Here, VADER is a little unsure wether the sentence is negative or neutral, although not positive, which is good. That is because \"ruins\" is considered in the lexicon as negative (-1.9), but as there are no other negative intensifiers, it is more neutral.\n",
    "```\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "```\n",
    " Here, VADER gives a correct interpretation of the sentence as being positive and neutral. That is because, although it recognizes \"ruins\" as being negative, it turns it to positive because of the intensified negator \"certainly not\". As there are no more intensifiers of sentiment, the sentence also oscilates torwards a neutral sentiment.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "```\n",
    " Here, there is a bit of misinterpretation. Although the sentence is completely neutral, given that is is just an information, VADER interprets the word \"lies\" as negative in its lexicon. That is because this word is a homonym to the infinitive verb \"lies\", which means \"to not tell the truth\", which induces a negative sentiment. This makes it interpret the sentence as having 28.6% of negative content, although it should be 100% neutral.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612\n",
    "```\n",
    "\n",
    " Here, VADER made the same mistake as INPUT SENTENCE 6, but instead of negative, it interpreted the sentence as being one third positive, although it should also be completely neutral. That is because it recognizes the word \"like\" as having a positive valence score in its lexicon, because it is a homonym to the verb \"like\" as in \"to like something\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef86aa",
   "metadata": {},
   "source": [
    "## Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c202678a-4f5b-44b8-9c85-1f816c64f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a38d19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'negative', 'text_of_tweet': 'I hate when pretty girls act mean cause bitch now you’re ugly.', 'tweet_url': 'https://twitter.com/raveneliseee/status/1763188901617819703'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_,tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86a941",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "677dc684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1443feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "\n",
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "def run_vader(textual_unit, \n",
    "              parts_of_speech_to_consider=None, lemmatize=False,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet,lemmatize=to_lemmatize)# run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "# use scikit-learn's classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72113c53",
   "metadata": {},
   "source": [
    "### 3.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d39123d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet: I hate when pretty girls act mean cause bitch now you’re ugly.\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: I hate when people take kindness for granted\n",
      "vader label: positive\n",
      "gold label: negative\n",
      "\n",
      "tweet: I really hate the fact that some people don’t have the patience to read and understand basic communication & instructions……  it’s unfair to humanity.\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: i hate feeling like i’m begging someone to do something they said they would. just keep your word please.\n",
      "vader label: positive\n",
      "gold label: negative\n",
      "\n",
      "tweet: People who do things and blame it on alcohol disgust me\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: People that are love obsessed disgust tf out of me, cause why do you always want to be in love(a relationship) are you possessed?\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: Your friend is literally shtting on your interest to your face after you expressed discomfort… I doubt theyd care this much about you\n",
      "vader label: positive\n",
      "gold label: negative\n",
      "\n",
      "tweet: Being an anxious ambitious girlie really sucks\n",
      "You can’t grow without discomfort but the uncertainty is triggering so then you’re just left in a constant state of anxiety\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: I wish people would simply do their part of the group assignment instead of displacing their discomfort and stealing time, health, & safety from others\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: He's a damn psychopath.\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: Damn. City boys are down bad\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: do i really deserve all this pain? am i that bad?\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: Nothing worse than a woman who blames her bad behavior on her zodiac sign\n",
      "vader label: positive\n",
      "gold label: negative\n",
      "\n",
      "tweet: foo fighters sucks\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: female friendships suck. Go argue with the wall\n",
      "vader label: negative\n",
      "gold label: negative\n",
      "\n",
      "tweet: Clean hydrogen is the Swiss Army Knife of decarbonization. @electric_H2is doing incredible work to make it affordable and more widely available.\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: Here’s a look at some of the highlights worth watching from late-night TV.\n",
      "vader label: positive\n",
      "gold label: neutral\n",
      "\n",
      "tweet: Be honest, does banana bread taste nice to you?\n",
      "vader label: positive\n",
      "gold label: neutral\n",
      "\n",
      "tweet: Deep brain stimulation (DBS) maps dysfunctional frontal cortical circuits in dystonia, Parkinson's disease, Tourette's syndrome, and OCD, which helped to guide stimulation sites in other patients\n",
      "@b_hollunder NingfeiL @andreashorn_ @netstim_org\n",
      "vader label: positive\n",
      "gold label: neutral\n",
      "\n",
      "tweet: A phenotypic screening platform for identifying chemical modulators of #astrocyte reactivity reveals histone deacetylase 3 (HDAC3) inhibitors as suppressors of pathological astrocyte reactivity\n",
      "@TesarLab @bllclayton\n",
      "vader label: negative\n",
      "gold label: neutral\n",
      "\n",
      "tweet: A molecularly defined amygdala-independent tetra-synaptic forebrain-to-hindbrain pathway for odor-driven innate fear and anxiety in mice\n",
      "vader label: negative\n",
      "gold label: neutral\n",
      "\n",
      "tweet: Local synaptic inhibition mediates cerebellar granule cell pattern separation and enables learned sensorimotor associations\n",
      "vader label: negative\n",
      "gold label: neutral\n",
      "\n",
      "tweet: As impressive as they seem, the latest computer brains usually fail at tasks that require generalizing concepts — tasks that come easily to humans.\n",
      "vader label: positive\n",
      "gold label: neutral\n",
      "\n",
      "tweet: A retrovirus — embedded in the DNA of jawed vertebrates — helps turn on production of a protein needed to insulate nerve fibers. Such insulation, called myelin, may have helped make speedy thoughts and complex brains possible.\n",
      "vader label: positive\n",
      "gold label: neutral\n",
      "\n",
      "tweet: Human have been eating hazelnuts for at least 6,000 years\n",
      "vader label: neutral\n",
      "gold label: neutral\n",
      "\n",
      "tweet: Today, artificial intelligence can identify potential drug targets and develop medicines from scratch. But whether the approach can craft drugs that help patients remains to be seen.\n",
      "vader label: positive\n",
      "gold label: neutral\n",
      "\n",
      "tweet: Sekazi Mtingwa was one of two key developers of the theory of intrabeam scattering, which has been applied in the building and operation of particle accelerators across the world.\n",
      "vader label: neutral\n",
      "gold label: neutral\n",
      "\n",
      "tweet: Video of a purported newborn white shark, taken by a drone off the coast of California, went viral, garnering over a million views and a spate of somewhat breathless news coverage.\n",
      "vader label: neutral\n",
      "gold label: neutral\n",
      "\n",
      "tweet: OpenAI wants to make a walking, talking humanoid robot smarter\n",
      "vader label: positive\n",
      "gold label: neutral\n",
      "\n",
      "tweet: How to check your measles vaccination status amid outbreak\n",
      "vader label: neutral\n",
      "gold label: neutral\n",
      "\n",
      "tweet: Unlike people, today's generative AI isn’t good at learning concepts that it can apply to new situations.\n",
      "vader label: negative\n",
      "gold label: neutral\n",
      "\n",
      "tweet: if i ever get to hear him sing everythingoes in person, all my discomfort would wash away\n",
      "vader label: negative\n",
      "gold label: positive\n",
      "\n",
      "tweet: you deserve love and kindness, not pain.\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: you will never understand how much i love this man and how down bad i am for him\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: i was kinda feeling bad eating this today but god it was so good\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: It's great to see all the innovative climate solutions featured this week.\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: enjoying a nice early lunch today😋😋\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: I’ve had a very productive morning working from home today so I deserve a good lunch lol\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: Im soooo glad i got this job because it means ill probably most likely be able to go to Mexico for semana santa 🥳which is a month from today lol\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: i've had lunch with a friend today and i'm having lunch with another friend tomorrow hhhhhhhh\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: sometimes when u dont style ur hair and let ur curls frizz out, it's nice\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: Good Night every one have nice dreamz🌹🌷\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: ouhgh...ppl are so nice to mme..\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: Love this post😂😂\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: it’s the way she looks at him after saying she loves him then how she leans to his direction so she can kiss him 🥹\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: Gm legends 💜\n",
      "Now, this guy is the definition of humble. What a legend 👊\n",
      "vader label: neutral\n",
      "gold label: positive\n",
      "\n",
      "tweet: just met babar. Such a nice humble cute guy\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: Monkeys getting grapes...notice what nice manners they have. 😊\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n",
      "tweet: Engineering a new mouse model of #autoimmunity, researchers have made strides in understanding the basis of a mysterious and rare autoimmune disorder named STAT3-GOF syndrome and identify several sex-specific differences in the disease. \n",
      "@SciSignal\n",
      "https://scim.ag/62R\n",
      "vader label: negative\n",
      "gold label: neutral\n",
      "\n",
      "tweet: I love their expressions of disgust 😂\n",
      "vader label: positive\n",
      "gold label: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_tweets = len(tweets)\n",
    "wrong_guesses = {}\n",
    "\n",
    "for i in range(total_tweets):\n",
    "    print(\"tweet:\",tweets[i])\n",
    "    print(\"vader label:\", all_vader_output[i])\n",
    "    print(\"gold label:\", gold[i])\n",
    "    if all_vader_output[i] != gold[i]:\n",
    "        category_tweets_dict = wrong_guesses.get(gold[i],{})\n",
    "        wrong_tweets_by_category = category_tweets_dict.get(all_vader_output[i],[])\n",
    "        wrong_tweets_by_category.append(tweets[i])\n",
    "        category_tweets_dict[all_vader_output[i]] = wrong_tweets_by_category\n",
    "        wrong_guesses[gold[i]] = category_tweets_dict\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f97cd14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.73      0.69        15\n",
      "     neutral       0.80      0.25      0.38        16\n",
      "    positive       0.61      0.89      0.72        19\n",
      "\n",
      "    accuracy                           0.64        50\n",
      "   macro avg       0.68      0.63      0.60        50\n",
      "weighted avg       0.68      0.64      0.60        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(gold,all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714e33c",
   "metadata": {},
   "source": [
    "#### Having a look at the metrics, this is what we can extract:\n",
    "* VADER has a good precision (80%) on neutral sentiment data, which means that most of the time it guesses the sentiment as being neutral, it is correct. On the other hand, it has a poor performance when we consider the recall, that is, a large number of sentences which were neutral were either recognized as being positive or negative. This indicates that it has to be trained more on neutral data. This might have happened because the neutral training data is largely composed of scientific news headlines, and sometimes the subject of the article can be either interpreted as negative (such as diseases) or positive (such as health treatment improvements).\n",
    "\n",
    "* VADER also shows a good recall on positive data. However, the precision is a bit low. This indicates that although it almost does not miss a positive labeled tweet, it assumes a good part of the tweets were positive when in fact they were neutral or negative. This indicates the need to improve training on positive sentiment data.\n",
    "\n",
    "* Lastly, VADER shows a similar performance in negative data as it did on positive, when it comes to precision. However, the recall is significantly worse, making it clear that there was a good part of tweets which were negative but were not recognized.\n",
    "\n",
    "#### Although the metrics observed gave hints about what needs improvement, in this classification problem it is unclear wether we need to improve recall or precision. That is because there is no consequence whatsoever of incorectly guessing a category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bce958",
   "metadata": {},
   "source": [
    "### 3.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab67d7e",
   "metadata": {},
   "source": [
    "### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1428efbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets guessed as positive:\n",
      "\n",
      "- I hate when people take kindness for granted\n",
      "- i hate feeling like i’m begging someone to do something they said they would. just keep your word please.\n",
      "- Your friend is literally shtting on your interest to your face after you expressed discomfort… I doubt theyd care this much about you\n",
      "- Nothing worse than a woman who blames her bad behavior on her zodiac sign\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for category,tweets in wrong_guesses[\"negative\"].items():\n",
    "    print(f\"Tweets guessed as {category}:\\n\")\n",
    "    for tweet in tweets:\n",
    "        print(\"-\",tweet)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33052ff1",
   "metadata": {},
   "source": [
    "#### In the sentences that VADER guessed as being negative, they were all incorrectly guessed as positive. In the first one, this could be attributed to the fact that it also contains the word \"kindness\", which has a really positive valence on the lexicon (2.3), and maybe that could be bringing the score up. Apart from the third sentence, which has the word \"friend\", all other tweets are guessed really innaccurately. That maybe can be caused by grammatical erros and poor harmony with the lexicon, so that VADER has not much data out of a sentence to correctly guess what sentiment it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b3621",
   "metadata": {},
   "source": [
    "### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88f9696b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets guessed as negative:\n",
      "\n",
      "- if i ever get to hear him sing everythingoes in person, all my discomfort would wash away\n",
      "\n",
      "Tweets guessed as neutral:\n",
      "\n",
      "- Gm legends 💜\n",
      "Now, this guy is the definition of humble. What a legend 👊\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for category,tweets in wrong_guesses[\"positive\"].items():\n",
    "    print(f\"Tweets guessed as {category}:\\n\")\n",
    "    for tweet in tweets:\n",
    "        print(\"-\",tweet)\n",
    "    print()\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d17269",
   "metadata": {},
   "source": [
    "#### The first sentence is guessed as negative. That can be attributed to the fact that it contains the work \"discomfort\", which is a negative word according to the lexicon. The other sentence is guessed as neutral, and this can maybe due to the fact that VADER does not recognize \"the definition of humble\" as a compliment, maybe because \"humble\" is not in the lexicon, although this sentence contains emojis which could give a hint to the sentence being positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e392af",
   "metadata": {},
   "source": [
    "### Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1981a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets guessed as positive:\n",
      "\n",
      "- Here’s a look at some of the highlights worth watching from late-night TV.\n",
      "- Be honest, does banana bread taste nice to you?\n",
      "- Deep brain stimulation (DBS) maps dysfunctional frontal cortical circuits in dystonia, Parkinson's disease, Tourette's syndrome, and OCD, which helped to guide stimulation sites in other patients\n",
      "@b_hollunder NingfeiL @andreashorn_ @netstim_org\n",
      "- As impressive as they seem, the latest computer brains usually fail at tasks that require generalizing concepts — tasks that come easily to humans.\n",
      "- A retrovirus — embedded in the DNA of jawed vertebrates — helps turn on production of a protein needed to insulate nerve fibers. Such insulation, called myelin, may have helped make speedy thoughts and complex brains possible.\n",
      "- Today, artificial intelligence can identify potential drug targets and develop medicines from scratch. But whether the approach can craft drugs that help patients remains to be seen.\n",
      "- OpenAI wants to make a walking, talking humanoid robot smarter\n",
      "\n",
      "Tweets guessed as negative:\n",
      "\n",
      "- A phenotypic screening platform for identifying chemical modulators of #astrocyte reactivity reveals histone deacetylase 3 (HDAC3) inhibitors as suppressors of pathological astrocyte reactivity\n",
      "@TesarLab @bllclayton\n",
      "- A molecularly defined amygdala-independent tetra-synaptic forebrain-to-hindbrain pathway for odor-driven innate fear and anxiety in mice\n",
      "- Local synaptic inhibition mediates cerebellar granule cell pattern separation and enables learned sensorimotor associations\n",
      "- Unlike people, today's generative AI isn’t good at learning concepts that it can apply to new situations.\n",
      "- Engineering a new mouse model of #autoimmunity, researchers have made strides in understanding the basis of a mysterious and rare autoimmune disorder named STAT3-GOF syndrome and identify several sex-specific differences in the disease. \n",
      "@SciSignal\n",
      "https://scim.ag/62R\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for category,tweets in wrong_guesses[\"neutral\"].items():\n",
    "    print(f\"Tweets guessed as {category}:\\n\")\n",
    "    for tweet in tweets:\n",
    "        print(\"-\",tweet)\n",
    "    print()\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa305a5",
   "metadata": {},
   "source": [
    "#### We can see that the neutral sentiment is the hardest one for VADER to guess precisely. That can be attributed to the fact that it takes the words inside the headlines (such as \"isn't good\", \"smarter\", \"impressive\") and retrieve their sentiment from the lexicon. This affects the sentence as a whole, as there are not many intensifiers or other attributes which make the sentiment of the sentence clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5171b20",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16c65c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "576662b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fe84b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_tweets_folder = cwd.joinpath('airlinetweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d9c21f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "print('this will print True if the folder exists:', \n",
    "      airline_tweets_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd25ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_files(airline_tweets_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bbe2cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831ef53",
   "metadata": {},
   "source": [
    "* Run VADER (as it is) on the set of airline tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3ba77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.63      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold = []\n",
    "for target in dataset.target:\n",
    "    gold.append(dataset.target_names[target])\n",
    "\n",
    "vader_output = []\n",
    "\n",
    "for tweet in dataset.data:\n",
    "    scores = run_vader(str(tweet),lemmatize=False)\n",
    "    vader_output.append(vader_output_to_label(scores))\n",
    "\n",
    "report = classification_report(gold,vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a2dbd",
   "metadata": {},
   "source": [
    "* Run VADER on the set of airline tweets after having lemmatized the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cafbe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.63      1750\n",
      "     neutral       0.60      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vader_output = []\n",
    "\n",
    "for tweet in dataset.data:\n",
    "    scores = run_vader(str(tweet),lemmatize=True)\n",
    "    vader_output.append(vader_output_to_label(scores))\n",
    "\n",
    "report = classification_report(gold,vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cad6a",
   "metadata": {},
   "source": [
    "* Run VADER on the set of airline tweets with only adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d9f2ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.21      0.34      1750\n",
      "     neutral       0.41      0.89      0.56      1515\n",
      "    positive       0.66      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.64      0.52      0.48      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vader_output = []\n",
    "\n",
    "for tweet in dataset.data:\n",
    "    scores = run_vader(str(tweet),lemmatize=False,parts_of_speech_to_consider=[\"ADJ\"])\n",
    "    vader_output.append(vader_output_to_label(scores))\n",
    "\n",
    "report = classification_report(gold,vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e8870",
   "metadata": {},
   "source": [
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "696856e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.21      0.34      1750\n",
      "     neutral       0.41      0.89      0.56      1515\n",
      "    positive       0.66      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.64      0.52      0.48      4755\n",
      "weighted avg       0.65      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vader_output = []\n",
    "\n",
    "for tweet in dataset.data:\n",
    "    scores = run_vader(str(tweet),lemmatize=True,parts_of_speech_to_consider=[\"ADJ\"])\n",
    "    vader_output.append(vader_output_to_label(scores))\n",
    "\n",
    "report = classification_report(gold,vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74fb395",
   "metadata": {},
   "source": [
    "* Run VADER on the set of airline tweets with only nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05988098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.14      0.23      1750\n",
      "     neutral       0.36      0.82      0.50      1515\n",
      "    positive       0.54      0.33      0.41      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.54      0.43      0.38      4755\n",
      "weighted avg       0.55      0.42      0.37      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vader_output = []\n",
    "\n",
    "for tweet in dataset.data:\n",
    "    scores = run_vader(str(tweet),lemmatize=False,parts_of_speech_to_consider=[\"NOUN\"])\n",
    "    vader_output.append(vader_output_to_label(scores))\n",
    "\n",
    "report = classification_report(gold,vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60817c",
   "metadata": {},
   "source": [
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3ff72ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.15      0.25      1750\n",
      "     neutral       0.36      0.82      0.50      1515\n",
      "    positive       0.53      0.32      0.40      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.53      0.43      0.38      4755\n",
      "weighted avg       0.54      0.42      0.37      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vader_output = []\n",
    "\n",
    "for tweet in dataset.data:\n",
    "    scores = run_vader(str(tweet),lemmatize=True,parts_of_speech_to_consider=[\"NOUN\"])\n",
    "    vader_output.append(vader_output_to_label(scores))\n",
    "\n",
    "report = classification_report(gold,vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44deee",
   "metadata": {},
   "source": [
    "* Run VADER on the set of airline tweets with only verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ea1d54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.29      0.42      1750\n",
      "     neutral       0.38      0.82      0.52      1515\n",
      "    positive       0.57      0.34      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.58      0.48      0.46      4755\n",
      "weighted avg       0.59      0.47      0.45      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vader_output = []\n",
    "\n",
    "for tweet in dataset.data:\n",
    "    scores = run_vader(str(tweet),lemmatize=False,parts_of_speech_to_consider=[\"VERB\"])\n",
    "    vader_output.append(vader_output_to_label(scores))\n",
    "\n",
    "report = classification_report(gold,vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27bb11",
   "metadata": {},
   "source": [
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b99384c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.29      0.42      1750\n",
      "     neutral       0.38      0.79      0.51      1515\n",
      "    positive       0.57      0.35      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.56      0.48      0.45      4755\n",
      "weighted avg       0.57      0.47      0.45      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vader_output = []\n",
    "\n",
    "for tweet in dataset.data:\n",
    "    scores = run_vader(str(tweet),lemmatize=True,parts_of_speech_to_consider=[\"VERB\"])\n",
    "    vader_output.append(vader_output_to_label(scores))\n",
    "\n",
    "report = classification_report(gold,vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d98ca1",
   "metadata": {},
   "source": [
    "#### It is possible to note that:\n",
    "\n",
    "* Lemmatization does not have a significant impact on the sentiment evaluation given by VADER. Sometimes, the scores go up or down a little bit, but in general the scores before and after lemmatization are quite similar.\n",
    "\n",
    "* Adjectives are the most important part of speech for sentiment analysis. That can be justified given that adjectives are generally responsible for giving the sentiment to a sentence. Take, for example, the phrase \"Something is good\". Here, \"something\" is a noun, \"is\" a verb, and \"good\" an adjective. Out of those 3 words, the only one which gives information on sentiment is \"good\", which is a positive adjective, while the others are neutral. For instance, if we change the adjective \"good\" to \"bad\", the sentiment of the phrase changes. \n",
    "\n",
    "* Although ajectives are the most important parts of speech, VADER's analysis performs better if we don't filter out other parts of speech (the first 2 experiments gave out the best results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafce24c",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eba8f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e52cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/temp/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/temp/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=2,tokenizer=nltk.word_tokenize,stop_words=stopwords.words('english'))\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "dbc3e174",
=======
   "execution_count": 51,
   "id": "dbc3e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bceb48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_tfidf, docs_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
    "    airline_tfidf,\n",
    "    dataset.target,\n",
    "    test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa9e71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts,\n",
    "    dataset.target,\n",
    "    test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50962b8",
   "metadata": {},
   "source": [
    "* TFIDF prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66a7c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(docs_train_tfidf,y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d57ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(docs_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4eb63892",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test_tfidf,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a08d160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84       348\n",
      "           1       0.82      0.69      0.75       301\n",
      "           2       0.83      0.84      0.84       302\n",
      "\n",
      "    accuracy                           0.81       951\n",
      "   macro avg       0.81      0.81      0.81       951\n",
      "weighted avg       0.81      0.81      0.81       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f7a50",
   "metadata": {},
   "source": [
    "* Bag-of-words prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b946bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(docs_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36444231",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f890e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6aa0ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       354\n",
      "           1       0.84      0.71      0.77       304\n",
      "           2       0.82      0.87      0.84       293\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.84      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5b760",
   "metadata": {},
   "source": [
    "* Changing min_df = 5, with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d08608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/temp/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/temp/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=5,tokenizer=nltk.word_tokenize,stop_words=stopwords.words('english'))\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(dataset.data)\n",
    "\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9c51f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf,\n",
    "    dataset.target,\n",
    "    test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91d4a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       357\n",
      "           1       0.86      0.69      0.76       310\n",
      "           2       0.82      0.88      0.85       284\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(docs_train,y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1b9ab",
   "metadata": {},
   "source": [
    "* Now with min_df = 10, with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "debbc82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/temp/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/temp/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=10,tokenizer=nltk.word_tokenize,stop_words=stopwords.words('english'))\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(dataset.data)\n",
    "\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16d58f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf,\n",
    "    dataset.target,\n",
    "    test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "11e3342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85       345\n",
      "           1       0.80      0.69      0.74       316\n",
      "           2       0.82      0.82      0.82       290\n",
      "\n",
      "    accuracy                           0.81       951\n",
      "   macro avg       0.81      0.80      0.80       951\n",
      "weighted avg       0.81      0.81      0.80       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(docs_train,y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918aa13d",
>>>>>>> Stashed changes
   "metadata": {},
   "source": [
    "- a. Given above\n",
    "- b. \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
