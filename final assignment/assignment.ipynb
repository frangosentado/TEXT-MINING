{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4fd7df-c0c2-4f54-aa55-b207f80a701b",
   "metadata": {},
   "source": [
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24000cc3-29d9-4e1d-b62c-5583558c56be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99496c40-105c-45a3-ab58-b9a077cf3f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a01c68be-9e34-4b46-87da-3517c315b830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "\n",
    "def run_vader(textual_unit, \n",
    "              parts_of_speech_to_consider=None, lemmatize=False,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9b8eae9-93a9-4811-a002-fb21fbc35618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8aea5ad-4faa-4568-b5ff-02aa2872c666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"./sentiment-topic-test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fbb6a4e-57c7-4303-b440-2f63f84e3613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gold = []\n",
    "vader_output = []\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2,tokenizer=nltk.word_tokenize,stop_words=stopwords.words('english')), 'name': 'Bag of Words, min_df=2'\n",
    "\n",
    "for row in test_dataset.itertuples():\n",
    "    gold_label = row[3]\n",
    "    sentence = row[2]\n",
    "    gold.append(str(gold_label))\n",
    "    scores = run_vader(str(sentence),lemmatize=False)\n",
    "    vader_output.append(vader_output_to_label(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "206bf52f-7bbc-42b4-8b35-2c6aed902853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report = classification_report(gold,vader_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d650fbf7-6484-42a6-9128-c27e6869ccdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'neutral', 'positive', 'positive', 'negative', 'neutral', 'negative', 'negative', 'neutral', 'positive']\n",
      "['positive', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'negative', 'negative']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         4\n",
      "     neutral       0.33      0.33      0.33         3\n",
      "    positive       0.20      0.33      0.25         3\n",
      "\n",
      "    accuracy                           0.20        10\n",
      "   macro avg       0.18      0.22      0.19        10\n",
      "weighted avg       0.16      0.20      0.17        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gold)\n",
    "print(vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb8f8c-7e87-435c-a381-d084868d8799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
