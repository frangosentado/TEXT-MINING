{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4fd7df-c0c2-4f54-aa55-b207f80a701b",
   "metadata": {},
   "source": [
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24000cc3-29d9-4e1d-b62c-5583558c56be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b8eae9-93a9-4811-a002-fb21fbc35618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8aea5ad-4faa-4568-b5ff-02aa2872c666",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# cwd = pathlib.Path.cwd()\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m airline_tweets_folder \u001b[38;5;241m=\u001b[39m cwd\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairlinetweets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m airline_dataset \u001b[38;5;241m=\u001b[39m load_files(airline_tweets_folder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cwd' is not defined"
     ]
    }
   ],
   "source": [
    "#getting the datasets\n",
    "from sklearn.datasets import load_files\n",
    "import pathlib\n",
    "\n",
    "# cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_dataset = load_files(airline_tweets_folder)\n",
    "# DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "# DATASET_ENCODING = \"ISO-8859-1\"\n",
    "\n",
    "# you'll need to download the db at https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "# train_dataset = pd.read_csv(\"./1milliontweets.csv\",encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "\n",
    "# test_dataset = pd.read_csv(\"./sentiment-topic-test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f4f6a",
   "metadata": {},
   "source": [
    "#### Using Multinomial Nayve-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb6a4e-57c7-4303-b440-2f63f84e3613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = test_dataset.get(\"text\").values\n",
    "golden_labels = test_dataset.get(\"sentiment\").values\n",
    "\n",
    "count_vec = CountVectorizer(min_df=1,tokenizer=nltk.word_tokenize,stop_words=stopwords.words('english'))\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_count_vec = count_vec.fit_transform(train_dataset.get(\"text\"))\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_count_vec)\n",
    "\n",
    "sentences_counts = count_vec.transform(sentences)\n",
    "sentences_tfidf = tfidf_transformer.fit_transform(sentences_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(train_tfidf,train_dataset.sentiment)\n",
    "\n",
    "pred = clf.predict(sentences_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb8f8c-7e87-435c-a381-d084868d8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_label(sentiment: float) -> str:\n",
    "    if sentiment > float(4/3):\n",
    "        if sentiment > float(2*4/3):\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    return \"negative\"\n",
    "        \n",
    "\n",
    "predicted_tolabel = []\n",
    "for sentence,(golden,predicted) in zip(sentences,zip(golden_labels,pred)):\n",
    "    print(sentence)\n",
    "    print(\"GOLDEN LABEL:\", golden)\n",
    "    print(predicted)\n",
    "    predicted_label = get_predicted_label(predicted)\n",
    "    predicted_tolabel.append(predicted_label)\n",
    "    print(\"PREDICTED:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ee0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(type(golden_labels.tolist()))\n",
    "results = classification_report(golden_labels,predicted_tolabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f46633",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ece522",
   "metadata": {},
   "source": [
    "#### Using a Support-Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "lin_classifier = svm.LinearSVC()\n",
    "lin_classifier.fit(train_tfidf,train_dataset.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa60555",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lin_classifier.predict(sentences_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3896c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = [get_predicted_label(number) for number in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_labels)\n",
    "print(golden_labels)\n",
    "results_svm = classification_report(golden_labels.tolist(),pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94539de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ff918",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427bafa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebulcao/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.ner import NERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f13d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "englishmodel = NERModel(\n",
    "        model_type=\"bert\",\n",
    "        model_name=\"dslim/bert-base-NER\",\n",
    "        use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b40978f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_test_dataset = pd.read_csv(\"./NER-test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39697004",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ner = ner_test_dataset.get(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb916ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f051108208d487a8b501560b12ab87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebulcao/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc04726b0054f7da1ca70d9b3139122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_output = englishmodel.predict(tokens_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a58448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'I': 'O'}],\n",
       " [{'would': 'O'}],\n",
       " [{\"n't\": 'O'}],\n",
       " [{'be': 'O'}],\n",
       " [{'caught': 'O'}],\n",
       " [{'dead': 'O'}],\n",
       " [{'watching': 'O'}],\n",
       " [{'the': 'O'}],\n",
       " [{'NFL': 'B-ORG'}],\n",
       " [{'if': 'O'}],\n",
       " [{'it': 'O'}],\n",
       " [{'were': 'O'}],\n",
       " [{\"n't\": 'O'}],\n",
       " [{'for': 'O'}],\n",
       " [{'Taylor': 'B-PER'}],\n",
       " [{'Swift': 'B-PER'}],\n",
       " [{'.': 'O'}],\n",
       " [{'Chris': 'B-PER'}],\n",
       " [{\"O'Donnell\": 'B-PER'}],\n",
       " [{'stated': 'O'}],\n",
       " [{'that': 'O'}],\n",
       " [{'while': 'O'}],\n",
       " [{'filming': 'O'}],\n",
       " [{'for': 'O'}],\n",
       " [{'this': 'O'}],\n",
       " [{'movie': 'O'}],\n",
       " [{',': 'O'}],\n",
       " [{'he': 'O'}],\n",
       " [{'felt': 'O'}],\n",
       " [{'like': 'O'}],\n",
       " [{'he': 'O'}],\n",
       " [{'was': 'O'}],\n",
       " [{'in': 'O'}],\n",
       " [{'a': 'O'}],\n",
       " [{'Toys': 'O'}],\n",
       " [{\"''\": 'O'}],\n",
       " [{'R': 'O'}],\n",
       " [{\"''\": 'O'}],\n",
       " [{'Us': 'O'}],\n",
       " [{'commercial': 'O'}],\n",
       " [{'.': 'O'}],\n",
       " [{'The': 'O'}],\n",
       " [{'whole': 'O'}],\n",
       " [{'game': 'O'}],\n",
       " [{'was': 'O'}],\n",
       " [{'a': 'O'}],\n",
       " [{'rollercoaster': 'O'}],\n",
       " [{'ride': 'O'}],\n",
       " [{',': 'O'}],\n",
       " [{'but': 'O'}],\n",
       " [{'Los': 'B-ORG'}],\n",
       " [{'Angeles': 'B-LOC'}],\n",
       " [{'Lakers': 'B-ORG'}],\n",
       " [{'ultimately': 'O'}],\n",
       " [{'persevered': 'O'}],\n",
       " [{'and': 'O'}],\n",
       " [{'won': 'O'}],\n",
       " [{'!': 'O'}],\n",
       " [{'Zendaya': 'B-ORG'}],\n",
       " [{'slayed': 'O'}],\n",
       " [{'in': 'O'}],\n",
       " [{'Dune': 'B-MISC'}],\n",
       " [{'2': 'O'}],\n",
       " [{',': 'O'}],\n",
       " [{'as': 'O'}],\n",
       " [{'she': 'O'}],\n",
       " [{'does': 'O'}],\n",
       " [{'in': 'O'}],\n",
       " [{'all': 'O'}],\n",
       " [{'her': 'O'}],\n",
       " [{'movies': 'O'}],\n",
       " [{'.': 'O'}],\n",
       " [{'While': 'O'}],\n",
       " [{'my': 'O'}],\n",
       " [{'favorite': 'O'}],\n",
       " [{'player': 'O'}],\n",
       " [{'was': 'O'}],\n",
       " [{'playing': 'O'}],\n",
       " [{'this': 'O'}],\n",
       " [{'match': 'O'}],\n",
       " [{'and': 'O'}],\n",
       " [{'started': 'O'}],\n",
       " [{'off': 'O'}],\n",
       " [{'strongggg': 'O'}],\n",
       " [{',': 'O'}],\n",
       " [{'it': 'O'}],\n",
       " [{'went': 'O'}],\n",
       " [{'downhill': 'O'}],\n",
       " [{'after': 'O'}],\n",
       " [{'Messi': 'B-PER'}],\n",
       " [{\"'s\": 'O'}],\n",
       " [{'injyry': 'O'}],\n",
       " [{'midgame': 'O'}],\n",
       " [{'.': 'O'}],\n",
       " [{'My': 'O'}],\n",
       " [{'uncle': 'O'}],\n",
       " [{\"'s\": 'O'}],\n",
       " [{'brother': 'O'}],\n",
       " [{\"'s\": 'O'}],\n",
       " [{'neighbor': 'O'}],\n",
       " [{\"'s\": 'O'}],\n",
       " [{'cat': 'O'}],\n",
       " [{\"'s\": 'O'}],\n",
       " [{'veterinarian': 'O'}],\n",
       " [{'David': 'B-PER'}],\n",
       " [{'reads': 'O'}],\n",
       " [{'the': 'O'}],\n",
       " [{'communist': 'B-MISC'}],\n",
       " [{'manifesto': 'O'}],\n",
       " [{'in': 'O'}],\n",
       " [{'his': 'O'}],\n",
       " [{'spare': 'O'}],\n",
       " [{'time': 'O'}],\n",
       " [{'.': 'O'}],\n",
       " [{'He': 'O'}],\n",
       " [{'said': 'O'}],\n",
       " [{'that': 'O'}],\n",
       " [{'The': 'O'}],\n",
       " [{'Great': 'O'}],\n",
       " [{'Gatsby': 'B-PER'}],\n",
       " [{'is': 'O'}],\n",
       " [{'the': 'O'}],\n",
       " [{'best': 'O'}],\n",
       " [{'novell': 'O'}],\n",
       " [{'ever': 'O'}],\n",
       " [{',': 'O'}],\n",
       " [{'and': 'O'}],\n",
       " [{'I': 'O'}],\n",
       " [{'was': 'O'}],\n",
       " [{'about': 'O'}],\n",
       " [{'to': 'O'}],\n",
       " [{'throw': 'O'}],\n",
       " [{'hands': 'O'}],\n",
       " [{'.': 'O'}],\n",
       " [{'I': 'O'}],\n",
       " [{'could': 'O'}],\n",
       " [{'not': 'O'}],\n",
       " [{'look': 'O'}],\n",
       " [{'away': 'O'}],\n",
       " [{'from': 'O'}],\n",
       " [{'this': 'O'}],\n",
       " [{'train': 'O'}],\n",
       " [{'wrck': 'O'}],\n",
       " [{'of': 'O'}],\n",
       " [{'a': 'O'}],\n",
       " [{'movie': 'O'}],\n",
       " [{',': 'O'}],\n",
       " [{'on': 'O'}],\n",
       " [{'February': 'O'}],\n",
       " [{'14th': 'O'}],\n",
       " [{'of': 'O'}],\n",
       " [{'all': 'O'}],\n",
       " [{'days': 'O'}],\n",
       " [{'.': 'O'}],\n",
       " [{'The': 'O'}],\n",
       " [{'film': 'O'}],\n",
       " [{'Everything': 'O'}],\n",
       " [{'Everywhere': 'O'}],\n",
       " [{'All': 'O'}],\n",
       " [{'At': 'O'}],\n",
       " [{'Once': 'O'}],\n",
       " [{'follows': 'O'}],\n",
       " [{'Evelyn': 'B-PER'}],\n",
       " [{'Wang': 'B-PER'}],\n",
       " [{',': 'O'}],\n",
       " [{'a': 'O'}],\n",
       " [{'woman': 'O'}],\n",
       " [{'drowning': 'O'}],\n",
       " [{'under': 'O'}],\n",
       " [{'the': 'O'}],\n",
       " [{'stress': 'O'}],\n",
       " [{'of': 'O'}],\n",
       " [{'her': 'O'}],\n",
       " [{'family': 'O'}],\n",
       " [{\"'s\": 'O'}],\n",
       " [{'failing': 'O'}],\n",
       " [{'laundromat': 'O'}],\n",
       " [{'.': 'O'}],\n",
       " [{'I': 'O'}],\n",
       " [{'just': 'O'}],\n",
       " [{'finished': 'O'}],\n",
       " [{'reading': 'O'}],\n",
       " [{'pride': 'O'}],\n",
       " [{'and': 'O'}],\n",
       " [{'prejudice': 'O'}],\n",
       " [{'which': 'O'}],\n",
       " [{'had': 'O'}],\n",
       " [{'me': 'O'}],\n",
       " [{'HOOOKED': 'O'}],\n",
       " [{'from': 'O'}],\n",
       " [{'the': 'O'}],\n",
       " [{'beginning': 'O'}],\n",
       " [{'.': 'O'}]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3253da",
   "metadata": {},
   "source": [
    "## Topic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf11a6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (0.70.0)\n",
      "Requirement already satisfied: numpy in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (1.24.3)\n",
      "Requirement already satisfied: requests in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (4.65.0)\n",
      "Requirement already satisfied: regex in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (2022.7.9)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (4.32.1)\n",
      "Requirement already satisfied: datasets in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (2.12.0)\n",
      "Requirement already satisfied: scipy in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (1.3.0)\n",
      "Requirement already satisfied: seqeval in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: tensorboard in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (2.16.2)\n",
      "Requirement already satisfied: tensorboardx in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (2.6.2.2)\n",
      "Requirement already satisfied: pandas in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (2.0.3)\n",
      "Requirement already satisfied: tokenizers in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (0.13.2)\n",
      "Requirement already satisfied: wandb>=0.10.32 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (0.16.5)\n",
      "Requirement already satisfied: streamlit in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (1.32.2)\n",
      "Requirement already satisfied: sentencepiece in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: filelock in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from transformers>=4.31.0->simpletransformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from transformers>=4.31.0->simpletransformers) (0.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from transformers>=4.31.0->simpletransformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from transformers>=4.31.0->simpletransformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from transformers>=4.31.0->simpletransformers) (0.3.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from wandb>=0.10.32->simpletransformers) (8.0.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from wandb>=0.10.32->simpletransformers) (3.1.42)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from wandb>=0.10.32->simpletransformers) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from wandb>=0.10.32->simpletransformers) (1.44.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from wandb>=0.10.32->simpletransformers) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from wandb>=0.10.32->simpletransformers) (4.25.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from requests->simpletransformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from requests->simpletransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from requests->simpletransformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from requests->simpletransformers) (2024.2.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from datasets->simpletransformers) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from datasets->simpletransformers) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from datasets->simpletransformers) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from datasets->simpletransformers) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from datasets->simpletransformers) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from datasets->simpletransformers) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from datasets->simpletransformers) (0.13.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from pandas->simpletransformers) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from pandas->simpletransformers) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from scikit-learn->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from scikit-learn->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (5.2.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (1.7.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (5.3.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (9.4.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (13.7.0)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (4.7.1)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (0.8.1b0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from streamlit->simpletransformers) (6.3.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from tensorboard->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from tensorboard->simpletransformers) (1.62.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from tensorboard->simpletransformers) (3.4.1)\n",
      "Requirement already satisfied: six>1.9 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from tensorboard->simpletransformers) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from tensorboard->simpletransformers) (2.2.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jinja2 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.21.1)\n",
      "Requirement already satisfied: toolz in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets->simpletransformers) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets->simpletransformers) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.33.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/bebulcao/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66c9a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebulcao/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ce9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# load only a sub-selection of the categories (3 in our case)\n",
    "categories = ['rec.sport.baseball', 'talk.politics.misc', 'talk.religion.misc'] \n",
    "\n",
    "# remove the headers, footers and quotes (to avoid overfitting)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=categories, random_state=42)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=categories, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed7094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    'text': newsgroups_train.data,\n",
    "    'labels': newsgroups_train.target\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': newsgroups_test.data,\n",
    "    'labels': newsgroups_test.target\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34478e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, dev = train_test_split(train_df, test_size=0.1, random_state=0, \n",
    "                               stratify=train_df[['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b3123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration # https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model \n",
    "model_args = ClassificationArgs()\n",
    "\n",
    "model_args.overwrite_output_dir=True # overwrite existing saved models in the same directory\n",
    "model_args.evaluate_during_training=True # to perform evaluation while training the model\n",
    "# (eval data should be passed to the training method)\n",
    "\n",
    "model_args.num_train_epochs=10 # number of epochs\n",
    "model_args.train_batch_size=32 # batch size\n",
    "model_args.learning_rate=4e-6 # learning rate\n",
    "model_args.max_seq_length=256 # maximum sequence length\n",
    "# Note! Increasing max_seq_len may provide better performance, but training time will increase. \n",
    "# For educational purposes, we set max_seq_len to 256.\n",
    "\n",
    "# Early stopping to combat overfitting: https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping\n",
    "model_args.use_early_stopping=True\n",
    "model_args.early_stopping_delta=0.01 # \"The improvement over best_eval_loss necessary to count as a better checkpoint\"\n",
    "model_args.early_stopping_metric='eval_loss'\n",
    "model_args.early_stopping_metric_minimize=True\n",
    "model_args.early_stopping_patience=2\n",
    "model_args.evaluate_during_training_steps=32 # how often you want to run validation in terms of training steps (or batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcde0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each epoch will have 41 steps.\n"
     ]
    }
   ],
   "source": [
    "# Checking steps per epoch\n",
    "steps_per_epoch = int(np.ceil(len(train) / float(model_args.train_batch_size)))\n",
    "print('Each epoch will have {:,} steps.'.format(steps_per_epoch)) # 64 steps = validating 2 times per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05cb07de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel('bert', 'bert-base-cased', num_labels=3, args=model_args, use_cuda=False) # CUDA is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eee06fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e801dd1e2551441a82ddd961a85ece38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebulcao/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/bebulcao/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/bebulcao/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550853fb6efa4ee498fbf0b9b0c4862e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0857ff476d54cea890e2ee59455682b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7787b4fb7bf4a5aaa0fd4efd9a78d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebulcao/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_model(train, eval_df\u001b[38;5;241m=\u001b[39mdev,average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/simpletransformers/classification/classification_model.py:630\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    622\u001b[0m     train_dataset,\n\u001b[1;32m    623\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mtrain_sampler,\n\u001b[1;32m    624\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    625\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    626\u001b[0m )\n\u001b[1;32m    628\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 630\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    631\u001b[0m     train_dataloader,\n\u001b[1;32m    632\u001b[0m     output_dir,\n\u001b[1;32m    633\u001b[0m     multi_label\u001b[38;5;241m=\u001b[39mmulti_label,\n\u001b[1;32m    634\u001b[0m     show_running_loss\u001b[38;5;241m=\u001b[39mshow_running_loss,\n\u001b[1;32m    635\u001b[0m     eval_df\u001b[38;5;241m=\u001b[39meval_df,\n\u001b[1;32m    636\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    638\u001b[0m )\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;66;03m# model_to_save.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# self.tokenizer.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/simpletransformers/classification/classification_model.py:995\u001b[0m, in \u001b[0;36mClassificationModel.train\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, test_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(\n\u001b[1;32m    987\u001b[0m         output_dir_current, optimizer, scheduler, model\u001b[38;5;241m=\u001b[39mmodel\n\u001b[1;32m    988\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mevaluate_during_training \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    991\u001b[0m     args\u001b[38;5;241m.\u001b[39mevaluate_during_training_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mevaluate_during_training_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    993\u001b[0m ):\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;66;03m# Only evaluate when single GPU otherwise metrics may not average well\u001b[39;00m\n\u001b[0;32m--> 995\u001b[0m     results, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_model(\n\u001b[1;32m    996\u001b[0m         eval_df,\n\u001b[1;32m    997\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mevaluate_during_training_verbose,\n\u001b[1;32m    998\u001b[0m         silent\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mevaluate_during_training_silent,\n\u001b[1;32m    999\u001b[0m         wandb_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1001\u001b[0m     )\n\u001b[1;32m   1003\u001b[0m     output_dir_current \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1004\u001b[0m         output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(global_step)\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39msave_eval_checkpoints:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/simpletransformers/classification/classification_model.py:1359\u001b[0m, in \u001b[0;36mClassificationModel.eval_model\u001b[0;34m(self, eval_df, multi_label, output_dir, verbose, silent, wandb_log, **kwargs)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_model_to_device()\n\u001b[0;32m-> 1359\u001b[0m result, model_outputs, wrong_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m   1360\u001b[0m     eval_df,\n\u001b[1;32m   1361\u001b[0m     output_dir,\n\u001b[1;32m   1362\u001b[0m     multi_label\u001b[38;5;241m=\u001b[39mmulti_label,\n\u001b[1;32m   1363\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1364\u001b[0m     silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[1;32m   1365\u001b[0m     wandb_log\u001b[38;5;241m=\u001b[39mwandb_log,\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1367\u001b[0m )\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mupdate(result)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/simpletransformers/classification/classification_model.py:1599\u001b[0m, in \u001b[0;36mClassificationModel.evaluate\u001b[0;34m(self, eval_df, output_dir, multi_label, prefix, verbose, silent, wandb_log, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multi_label:\n\u001b[1;32m   1597\u001b[0m         preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1599\u001b[0m result, wrong \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   1600\u001b[0m     preds, model_outputs, out_label_ids, eval_examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1601\u001b[0m )\n\u001b[1;32m   1602\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m eval_loss\n\u001b[1;32m   1603\u001b[0m results\u001b[38;5;241m.\u001b[39mupdate(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/simpletransformers/classification/classification_model.py:1869\u001b[0m, in \u001b[0;36mClassificationModel.compute_metrics\u001b[0;34m(self, preds, model_outputs, labels, eval_examples, multi_label, **kwargs)\u001b[0m\n\u001b[1;32m   1867\u001b[0m         extra_metrics[metric] \u001b[38;5;241m=\u001b[39m func(labels, model_outputs)\n\u001b[1;32m   1868\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1869\u001b[0m         extra_metrics[metric] \u001b[38;5;241m=\u001b[39m func(labels, preds)\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_label:\n\u001b[1;32m   1872\u001b[0m     threshold_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "_, history = model.train_model(train, eval_df=dev,average='macro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f14f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
